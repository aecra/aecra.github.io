
    
    
    
    
    
    
    [{"content":"Description First of all, if you use Ctrl + C to force termination during the build of Podman, the containers in the build will be left in Podman, which will not only take up space but also affect the deletion of useless images.\nThere were forced interruptions on many previous builds, so my problem was more serious before I discovered it, otherwise one or two images that could not be deleted would not have attracted attention.\nMy images list：\nREPOSITORY TAG IMAGE ID CREATED SIZE \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 6b97c9f7f89f 2 days ago 1.67 GB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 37616f79cf88 2 days ago 992 MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; e5028987de23 2 days ago 1.67 GB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 29ebb80d809d 7 days ago 992 MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 30994a0dd3f6 7 days ago 1.67 GB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; eea90f9d3610 7 days ago 992 MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; e2f2eb80c593 7 days ago 1.67 GB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 339be923556e 7 days ago 930 MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; bc390bc57574 8 days ago 930 MB docker.io/library/golang 1.18.4 e3c0472b1b62 4 weeks ago 988 MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 57e0342b3532 5 weeks ago 991 MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 8b5845455f80 5 weeks ago 1.67 GB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 803d3146f7dd 7 weeks ago 1 GB docker.io/library/alpine latest e66264b98777 2 months ago 5.82 MB docker.io/library/golang 1.17.5 276895edf967 7 months ago 963 MB docker.io/library/node 16.13.0 5964aa70c11d 9 months ago 928 MB When using podman image prune to delete useless images, there are still images left as above, then I want to delete a single image, as follows to delete the first image in the list.\npodman rmi 6b97c9f7f89f In this case, an error will be reported:\nError: 1 errors occurred: * Image used by b63f5d5c99dd8bbb17510f58a9dfa2fe5b3a71ba57257aec6aa53c1ec4366db8: image is in use by a container But this container does not exist using pdoman ps -a to see.\nAfter some searching, I found that using podman ps --all --storage I can query the currently stored containers and images, which contain the containers left behind during the build process due to forced termination, as follows.\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f735cc1062a7 docker.io/library/golang:1.17.5 buildah 7 weeks ago storage golang-working-container fe95fad6d0ec docker.io/library/98ee13c4afe4a5a88fb6e3464a90ea3838e62806e9e5e025bedaaae2db25c653-tmp:latest buildah 7 weeks ago storage 8e860db1f8ded07d0d7083ee05ab5c17a7d41519b04ef3127bd3921f8b3df6ff-working-containerbf7d43b6e1de docker.io/library/b1d4f01a06501868672f31dee8be2506c18dbe938a0a12a2edb1e99fb1cb0b1d-tmp:latest buildah 7 weeks ago storage 803d3146f7ddce32b8f2e9f4f9ec7d51607f5b3ddc618758aa55634d0aa1576e-working-containere5fba2ea7216 docker.io/library/golang:1.17.5 buildah 7 weeks ago storage golang-working-container-1 7b94635d2311 docker.io/library/98ee13c4afe4a5a88fb6e3464a90ea3838e62806e9e5e025bedaaae2db25c653-tmp:latest buildah 7 weeks ago storage 8e860db1f8ded07d0d7083ee05ab5c17a7d41519b04ef3127bd3921f8b3df6ff-working-container-1 3297183ec5dd docker.io/library/b1d4f01a06501868672f31dee8be2506c18dbe938a0a12a2edb1e99fb1cb0b1d-tmp:latest buildah 7 weeks ago storage 803d3146f7ddce32b8f2e9f4f9ec7d51607f5b3ddc618758aa55634d0aa1576e-working-container-1 # There are very many more items in the back, and I have only listed some of them. Now the task is how to solve this garbage data.\nSolution Since Podman is just a container management tool that uses buildah to build images, we can use buildah directly to clean up the containers left over from the build process, using the following method.\nbuildah rm --all This method does not delete the images that are shown in podman iamges, because these images are given to podman, so you can safely delete them.\n","date":"2022-08-16T10:43:50Z","image":"https://static.aecra.cn/cover/Podman-build-container-residual-problem-when-the-build-process-is-forced-to-terminate.png","permalink":"https://www.aecra.cn/en/article/podman-build-container-residual-problem-when-the-build-process-is-forced-to-terminate/","title":"Podman build container residual problem when the build process is forced to terminate"},{"content":"Why should we build our own gateway？ First of all, to clarify the purpose of the gateway, I use the gateway completely to unifiedly manage the various background interfaces. Before building my own gateway my background interfaces are very scattered, some are deployed on the HTTP event trigger of the cloud function, some are deployed on the Web cloud function of the cloud function, some are on the API gateway of Tencent cloud, some are on the nginx proxy. This time I build my own gateway mainly to organize the interfaces provided, and at the same time clean up unnecessary configuration(mainly nginx).\nIf the above reasons alone are not enough to need to toss themselves again, but Tencent these days canceled the free quota of cloud functions, and since next month as long as the use will deduct the basic service fee of 12.8 yuan, Are you kinding me? Since services can be run using cloud functions, they can also be run in docker, so migrate all services in cloud functions to docker on the server. PS: Since the cloud function does not support container deployment at the beginning, some old services are directly bound to the cloud vendor\u0026rsquo;s services, so these old services will be temporarily stopped.\nStart Kong Gateway（docker） kong is still stable as an open source gateway service, here is how to deploy Kong, note the modification of PASSWORD in it.\nStart the database, which serves Kong.\ndocker run -d --name kong-database \\ --network=kong-net \\ -p 5432:5432 \\ -e \u0026#34;POSTGRES_USER=kong\u0026#34; \\ -e \u0026#34;POSTGRES_DB=kong\u0026#34; \\ -e \u0026#34;POSTGRES_PASSWORD=kongpass\u0026#34; \\ postgres:9.6 Initialize the configuration, here configure the database.\ndocker run --rm --network=kong-net \\ -e \u0026#34;KONG_DATABASE=postgres\u0026#34; \\ -e \u0026#34;KONG_PG_HOST=kong-database\u0026#34; \\ -e \u0026#34;KONG_PG_PASSWORD=kongpass\u0026#34; \\ kong/kong-gateway:2.8.1.0-alpine kong migrations bootstrap Start the Kong gateway, since we will be using the nginx proxy, SSL will be configured in nginx, and we are using the community edition, so just use the 8001 and 8000 ports.\ndocker run -d --name kong-gateway \\ --network=kong-net \\ -e \u0026#34;KONG_DATABASE=postgres\u0026#34; \\ -e \u0026#34;KONG_PG_HOST=kong-database\u0026#34; \\ -e \u0026#34;KONG_PG_USER=kong\u0026#34; \\ -e \u0026#34;KONG_PG_PASSWORD=kongpass\u0026#34; \\ -e \u0026#34;KONG_ADMIN_LISTEN=0.0.0.0:8001\u0026#34; \\ -p 8000:8000 \\ -p 8001:8001 \\ kong/kong-gateway:2.8.1.0-alpine :8000：For providing gateway services. :8001：The administrative API used to configure the Kong listener. Now that we have Kong configured, we launch a UI interface to help us manage it.\nStart konga konga is a third-party GUI administration page.\nWe also use the docker configuration.\ndocker run -p 1337:1337 \\ --network kong-net \\ --name konga \\ -e \u0026#34;NODE_ENV=production\u0026#34; \\ -e \u0026#34;TOKEN_SECRET=fdasfeag34agft\u0026#34; \\ pantsel/konga Once started, simply open port 1337 to configure our kong gateway via the UI.\nThe first time we enter, we need to fill in the kong configuration interface, because the above services are added to the kong-net network, so we only need to fill in http://kong-gateway:8001.\nConfig nginx Since we need to use nginx to configure the SSL service, we should modify the nginx configuration here.\nserver { listen 80; server_name hostname; return 301 https://hostname; } server { listen 443 ssl; server_name hostname; location / { proxy_pass http://localhost:8000; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_redirect off; } ssl_certificate /config/nginx/cert/fullchain.cer; ssl_certificate_key /config/nginx/cert/cert.key; ssl_session_timeout 1d; ssl_session_cache shared:MozSSL:10m; # about 40000 sessions ssl_session_tickets off; # intermediate configuration ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384; ssl_prefer_server_ciphers off; # HSTS (ngx_http_headers_module is required) (63072000 seconds) add_header Strict-Transport-Security \u0026#34;max-age=63072000\u0026#34; always; } Add Services In Kong, a distinction needs to be made between services and routes. services are the access settings to the back-end services. routes are the services provided by the gateway to the outside world. A service can have multiple routes; while a route can have only one service.\nSince all my services are on one server, I joined them to the same network as kong for security reasons. So when we configure service, we only need to fill in the container name in the HOST field, and we don\u0026rsquo;t need to add port mapping to improve security.\nTodo: I should have filled in some more detailed instructions on how to add the service here, but I am bored to write it, so I\u0026rsquo;ll leave it, if you have any questions you can concat me.\n","date":"2022-05-20T19:31:35Z","image":"https://static.aecra.cn/cover/build-a-gateway-by-kong.png","permalink":"https://www.aecra.cn/en/article/build-a-gateway-by-kong/","title":"Build a gateway by kong."},{"content":"Reasons for Vmware Virtual Machine Development Windows development is sometimes so unsatisfactory, especially when the services developed need to run in Linux, and there are always some inexplicable problems if you develop in Windows, so it is necessary to develop in Linux.\nThere are several options for using Linux development environment in Windows, such as WSL2, Hyper-V, VitualBox and Vmware. WSL2 is so tied to Windows that many configurations cannot be done as you wish. Hyper-V is more flexible, but it takes a long time to start up and shut down, and its performance is not very good. The more prominent problem is that I still don\u0026rsquo;t know how the network topology is, which causes a lot of problems for practical use. VirtualBox has not been used, but because of its open source and free reasons may switch to this in the future. Vmware is the best independent, the configuration and storage of virtual machines are saved in the form of separate files, which is convenient for migration and backup, and its network configuration is more flexible, so it is the first choice for Linux development.\nVmware Virtual Machine Startup and Shutdown Here a start-up script and a shutdown script are created, both of which read as follows.\nStart-up script:\n\u0026#34;C:\\Program Files (x86)\\VMware\\VMware Workstation\\vmrun.exe\u0026#34; start \u0026#34;D:\\virtual machine\\ubuntu20.04\\Ubuntu 20.04 64 位.vmx\u0026#34; nogui Shutdown script:\n\u0026#34;C:\\Program Files (x86)\\VMware\\VMware Workstation\\vmrun.exe\u0026#34; stop \u0026#34;D:\\virtual machine\\ubuntu20.04\\Ubuntu 20.04 64 位.vmx\u0026#34; soft Here, we use a GUI-free startup and enable the ssh service to connect remotely, so that we can connect to the VM remotely via ssh for development.\nSenseless operation of Vmware virtual machines We can use the Startup folder or Scheduled Tasks to set the boot-up in Windows, but here we use Group Policy.\nTo set up.\nwin + R to open Run. Type gpedit.msc to open the Local Group Policy Editor. Find User Configuration -\u0026gt; Windows Settings -\u0026gt; Scripts (Logon/Logoff). Add the scripts in the previous summary to Logon and Logoff respectively. After rebooting and shutting down, you can find that the virtual machine is running without any sense.\n","date":"2022-02-13T20:19:13Z","image":"https://static.aecra.cn/cover/VMware-development-virtual-machine-starts-and-shuts-down-with-the-system.jpg","permalink":"https://www.aecra.cn/en/article/vmware-development-virtual-machine-starts-and-shuts-down-with-the-system/","title":"VMware development virtual machine starts and shuts down with the system."},{"content":"This time I want to use ubuntu20.04 on VMware as my workspace. But I don’t want to move and backup my files frequently. So I need to find a way to mount my host files to virtual machine.\nFirst I need to install VMware-tools to my virtual machine. With some reasons, I can’t install it autoly( The button is grey). So use the next bash command to install it.\nsudo apt install open-vm-tools-desktop Then close the virtual machine. Select VM\u0026gt;Settings. On the Options tab, select Shared Folders. Select a folder sharing option. Click Add to add a shared folder. I add a folder named workspace.\nNow you can start this virtual mechine.\nOpen a terminal. use id \u0026lt;user\u0026gt; to get your gid and uid. My git and gid are 1000.\nNow We can mount the workspace folder to any folder in virtual machine through next command.\nsudo vmhgfs-fuse .host:/workspace ~/workspace -o allow_other -o uid=1000 -o gid=1000 Finally we can use this virtual machine to develop our projects.\n","date":"2022-01-22T15:25:34Z","image":"https://static.aecra.cn/cover/How-to-mount-host-folder-in-VMware-20.04-on-Windows.png","permalink":"https://www.aecra.cn/en/article/how-to-mount-host-folder-in-vmware-20.04-on-windows/","title":"How to mount host folder in VMware 20.04 on Windows."},{"content":"Introduction Nginx is a lightweight HTTP server that uses an event-driven, asynchronous, non-blocking processing framework, which gives it excellent IO performance and is often used for server-side reverse proxying and load balancing.\nQuick Start Install nginx\nsudo apt-get install nginx Uninstall nginx\nsudo apt-get uninstall nginx Start nginx\nnginx Stop nginx\nnginx -s stop Hot restart nginx\nnginx -s reload install in CentOS Install compilation tools and library files yum -y install make zlib zlib-devel gcc-c++ libtool openssl openssl-devel Install Download PCRE PCRE is used to enable Nginx to support Rewrite.\nThere are two very problematic areas here, the first is the version of PCRE to install, in PCRE official website we learned that PCRE2 has been released in 2015, according to the law of software updates, now NGINX should be able to support PCRE2, but after the actual installation trial and error found that it does not support PCRE2, and on StackOverflow it even says you need to use PCRE.\nThe second problem is the compilation parameters when compiling NGINX, -with-pcre and -with-pcre= are added without any problem, the problem lies in the directory of -with-pcre=, as the title of this section says, it is not possible to install PCRE normally and assign the library directory to NGINX, when specifying both When specifying both openssl and pcre, you must specify the source code of both projects, because the openssl system is already available, so we need to specify the source code of pcre, the final version of which is 8.45 after it no longer supports version 1.\nDownload PCRE source files，Download Links。\nThen transfer the file to the server.\nUnzip the code package.\nunzip pcre-8.45.zip Install Nginx Download Nginx，Download Links 。\ncd /usr/local/src/ wget https://nginx.org/download/nginx-1.20.1.tar.gz Unzip the installation package\ntar zxvf nginx-1.20.1.tar.gz Compile and install\n# Adding Users and Groups groupadd www useradd -g www www # Go to the installation directory cd nginx-1.20.1/ # Configuration ./configure \\ --user=www \\ --group=www \\ --prefix=/usr/local/nginx \\ --with-stream \\ --with-stream_ssl_preread_module \\ --with-http_ssl_module \\ --with-http_stub_status_module \\ --with-http_realip_module \\ --with-http_dav_module \\ --with-http_gzip_static_module \\ --with-http_v2_module \\ --with-pcre \\ --with-pcre=/usr/local/src/pcre-8.45 # Compile and install make \u0026amp;\u0026amp; make install Modify configuration Set boot-up Create the nginx.service file in the system services directory\nvi /usr/lib/systemd/system/nginx.service Write in the following\n[Unit] Description=nginx After=network.target [Service] Type=forking ExecStart=/usr/local/nginx/sbin/nginx ExecReload=/usr/local/nginx/sbin/nginx -s reload ExecStop=/usr/local/nginx/sbin/nginx -s quit PrivateTmp=true [Install] WantedBy=multi-user.target [Unit]: name of the service\nDescription: Description of the service\nAfter: Describe the service category\n[Service]: Setting of service operation parameters\nType=forking: It is the form that runs in the background\nExecStart Specific run commands for the service\nExecReload for the reboot command\nExecStop For the stop command\nPrivateTmp=True Indicates that a separate temporary space is allocated to the service\nCaution.[Service] requires absolute paths for all start, restart and stop commands.\n[Install] Settings related to service installation under runlevel can be set to multi-user, i.e. system runlevel is 3\nSet boot-up\nsystemctl enable nginx.service A reboot of the server is required after this step for successful configuration.\nGeneral Catalog (in docker) /etc/nginx/nginx.conf Configuration file path /usr/share/nginx/html The default root directory of the server /var/log/nginx The default log directory of the nginx server Configuration file structure ... #Global block events { #events block ... } http #http block { ... server #server blocks { ... location [PATTERN] #location blocks { ... } location [PATTERN] { ... } } server { ... } ... } Global block：Configure directives that affect nginx globally. There are generally user groups running the nginx server, nginx process pid storage paths, log storage paths, configuration file introduction, number of worker processes allowed to be generated, etc. events block：Configure network connections that affect the nginx server or to users. There is a maximum number of connections per process, which event-driven model is chosen to handle connection requests, whether to allow multiple network connections to be accepted at the same time, turn on serialization of multiple network connections, and so on. http block：Multiple servers can be nested. yong\u0026rsquo;y configures proxy, cache, log definition, and most other features and third-party module block configurations. Such as file introduction, mime-type definition, log customization, whether to use sendfile to transfer files, connection timeout, number of single connection requests, etc. server blocks：Configure the parameters of the virtual host, which can have multiple servers in one http. location blocks：Configure the routing of requests, and the processing of various pages. Official documentation: Link\n","date":"2021-07-03T08:50:45Z","image":"https://static.aecra.cn/cover/nginx-getting-started-guide.png","permalink":"https://www.aecra.cn/en/article/nginx-getting-started-guide/","title":"Nginx getting started guide."},{"content":"Getting Started The command you just ran Congratulations! You have started the container for this tutorial! Let\u0026rsquo;s first explain the command that you just ran. In case you forgot, here\u0026rsquo;s the command:\ndocker run -d -p 80:80 docker/getting-started You\u0026rsquo;ll notice a few flags being used. Here\u0026rsquo;s some more info on them:\n-d - run the container in detached mode (in the background) -p 80:80 - map port 80 of the host to port 80 in the container docker/getting-started - the image to use You can combine single character flags to shorten the full command. As an example, the command above could be written as: docker run -dp 80:80 docker/getting-started\nThe Docker Dashboard Before going too far, we want to highlight the Docker Dashboard, which gives you a quick view of the containers running on your machine. It gives you quick access to container logs, lets you get a shell inside the container, and lets you easily manage container lifecycle (stop, remove, etc.).\nTo access the dashboard, follow the instructions for either Mac or Windows. If you open the dashboard now, you will see this tutorial running! The container name (jolly_bouman below) is a randomly created name. So, you\u0026rsquo;ll most likely have a different name.\nWhat is a container? Now that you\u0026rsquo;ve run a container, what is a container? Simply put, a container is simply another process on your machine that has been isolated from all other processes on the host machine. That isolation leverages kernel namespaces and cgroups, features that have been in Linux for a long time. Docker has worked to make these capabilities approachable and easy to use.\nIf you\u0026rsquo;d like to see how containers are built from scratch, Liz Rice from Aqua Security has a fantastic talk in which she creates a container from scratch in Go. While she makes a simple container, this talk doesn\u0026rsquo;t go into networking, using images for the filesystem, and more. But, it gives a fantastic deep dive into how things are working.\nWhat is a container image? When running a container, it uses an isolated filesystem. This custom filesystem is provided by a container image. Since the image contains the container\u0026rsquo;s filesystem, it must contain everything needed to run an application - all dependencies, configuration, scripts, binaries, etc. The image also contains other configuration for the container, such as environment variables, a default command to run, and other metadata.\nWe\u0026rsquo;ll dive deeper into images later on, covering topics such as layering, best practices, and more.\nIf you\u0026rsquo;re familiar with chroot, think of a container as an extended version of chroot. The filesystem is simply coming from the image. But, a container adds additional isolation not available when simply using chroot.\nOur Application For the rest of this tutorial, we will be working with a simple todo list manager that is running in Node.js. If you\u0026rsquo;re not familiar with Node.js, don\u0026rsquo;t worry! No real JavaScript experience is needed!\nAt this point, your development team is quite small and you\u0026rsquo;re simply building an app to prove out your MVP (minimum viable product). You want to show how it works and what it\u0026rsquo;s capable of doing without needing to think about how it will work for a large team, multiple developers, etc.\nGetting our App Before we can run the application, we need to get the application source code onto our machine. For real projects, you will typically clone the repo. But, for this tutorial, we have created a ZIP file containing the application.\nDownload the ZIP. Open the ZIP file and make sure you extract the contents.\nOnce extracted, use your favorite code editor to open the project. If you\u0026rsquo;re in need of an editor, you can use Visual Studio Code. You should see the package.json and two subdirectories (src and spec).\nBuilding the App\u0026rsquo;s Container Image In order to build the application, we need to use a Dockerfile. A Dockerfile is simply a text-based script of instructions that is used to create a container image. If you\u0026rsquo;ve created Dockerfiles before, you might see a few flaws in the Dockerfile below. But, don\u0026rsquo;t worry! We\u0026rsquo;ll go over them.\nCreate a file named Dockerfile in the same folder as the file package.json with the following contents.\nFROM node:12-alpine RUN apk add --no-cache python g++ make WORKDIR /app COPY . . RUN yarn install --production CMD [\u0026#34;node\u0026#34;, \u0026#34;src/index.js\u0026#34;] Please check that the file Dockerfile has no file extension like .txt. Some editors may append this file extension automatically and this would result in an error in the next step.\nIf you haven\u0026rsquo;t already done so, open a terminal and go to the app directory with the Dockerfile. Now build the container image using the docker build command.\ndocker build -t getting-started . This command used the Dockerfile to build a new container image. You might have noticed that a lot of \u0026ldquo;layers\u0026rdquo; were downloaded. This is because we instructed the builder that we wanted to start from the node:12-alpine image. But, since we didn\u0026rsquo;t have that on our machine, that image needed to be downloaded.\nAfter the image was downloaded, we copied in our application and used yarn to install our application\u0026rsquo;s dependencies. The CMD directive specifies the default command to run when starting a container from this image.\nFinally, the -t flag tags our image. Think of this simply as a human-readable name for the final image. Since we named the image getting-started, we can refer to that image when we run a container.\nThe . at the end of the docker build command tells that Docker should look for the Dockerfile in the current directory.\nStarting an App Container Now that we have an image, let\u0026rsquo;s run the application! To do so, we will use the docker run command (remember that from earlier?).\nStart your container using the docker run command and specify the name of the image we just created:\ndocker run -dp 3000:3000 getting-started Remember the -d and -p flags? We\u0026rsquo;re running the new container in \u0026ldquo;detached\u0026rdquo; mode (in the background) and creating a mapping between the host\u0026rsquo;s port 3000 to the container\u0026rsquo;s port 3000. Without the port mapping, we wouldn\u0026rsquo;t be able to access the application.\nAfter a few seconds, open your web browser to http://localhost:3000. You should see our app!\nGo ahead and add an item or two and see that it works as you expect. You can mark items as complete and remove items. Your frontend is successfully storing items in the backend! Pretty quick and easy, huh?\nAt this point, you should have a running todo list manager with a few items, all built by you! Now, let\u0026rsquo;s make a few changes and learn about managing our containers.\nIf you take a quick look at the Docker Dashboard, you should see your two containers running now (this tutorial and your freshly launched app container)!\nRecap In this short section, we learned the very basics about building a container image and created a Dockerfile to do so. Once we built an image, we started the container and saw the running app!\nNext, we\u0026rsquo;re going to make a modification to our app and learn how to update our running application with a new image. Along the way, we\u0026rsquo;ll learn a few other useful commands.\nUpdating our App As a small feature request, we\u0026rsquo;ve been asked by the product team to change the \u0026ldquo;empty text\u0026rdquo; when we don\u0026rsquo;t have any todo list items. They would like to transition it to the following:\nYou have no todo items yet! Add one above!\nPretty simple, right? Let\u0026rsquo;s make the change.\nUpdating our Source Code In the src/static/js/app.js file, update line 56 to use the new empty text.\n- \u0026lt;p className=\u0026#34;text-center\u0026#34;\u0026gt;No items yet! Add one above!\u0026lt;/p\u0026gt; + \u0026lt;p className=\u0026#34;text-center\u0026#34;\u0026gt;You have no todo items yet! Add one above!\u0026lt;/p\u0026gt; Let\u0026rsquo;s build our updated version of the image, using the same command we used before.\ndocker build -t getting-started . Let\u0026rsquo;s start a new container using the updated code.\ndocker run -dp 3000:3000 getting-started Uh oh! You probably saw an error like this (the IDs will be different):\ndocker: Error response from daemon: driver failed programming external connectivity on endpoint laughing_burnell (bb242b2ca4d67eba76e79474fb36bb5125708ebdabd7f45c8eaf16caaabde9dd): Bind for 0.0.0.0:3000 failed: port is already allocated. So, what happened? We aren\u0026rsquo;t able to start the new container because our old container is still running. The reason this is a problem is because that container is using the host\u0026rsquo;s port 3000 and only one process on the machine (containers included) can listen to a specific port. To fix this, we need to remove the old container.\nReplacing our Old Container To remove a container, it first needs to be stopped. Once it has stopped, it can be removed. We have two ways that we can remove the old container. Feel free to choose the path that you\u0026rsquo;re most comfortable with.\nRemoving a container using the CLI Get the ID of the container by using the docker ps command. Or you can use docker ps -a to see all the container with closed.\ndocker ps Use the docker stop command to stop the container.\n# Swap out \u0026lt;the-container-id\u0026gt; with the ID from docker ps docker stop \u0026lt;the-container-id\u0026gt; Once the container has stopped, you can remove it by using the docker rm command.\ndocker rm \u0026lt;the-container-id\u0026gt; You can stop and remove a container in a single command by adding the \u0026ldquo;force\u0026rdquo; flag to the docker rm command. For example: docker rm -f \u0026lt;the-container-id\u0026gt;\nRemoving a container using the Docker Dashboard If you open the Docker dashboard, you can remove a container with two clicks! It\u0026rsquo;s certainly much easier than having to look up the container ID and remove it.\nWith the dashboard opened, hover over the app container and you\u0026rsquo;ll see a collection of action buttons appear on the right.\nClick on the trash can icon to delete the container.\nConfirm the removal and you\u0026rsquo;re done!\nStarting our updated app container Now, start your updated app.\ndocker run -dp 3000:3000 getting-started Refresh your browser on http://localhost:3000 and you should see your updated help text!\nRecap- While we were able to build an update, there were two things you might have noticed:\nAll of the existing items in our todo list are gone! That\u0026rsquo;s not a very good app! We\u0026rsquo;ll talk about that shortly. There were a lot of steps involved for such a small change. In an upcoming section, we\u0026rsquo;ll talk about how to see code updates without needing to rebuild and start a new container every time we make a change. Before talking about persistence, we\u0026rsquo;ll quickly see how to share these images with others.\nSharing our App Now that we\u0026rsquo;ve built an image, let\u0026rsquo;s share it! To share Docker images, you have to use a Docker registry. The default registry is Docker Hub and is where all of the images we\u0026rsquo;ve used have come from.\nCreate a Repo To push an image, we first need to create a repo on Docker Hub.\nGo to Docker Hub and log in if you need to.\nClick the Create Repository button.\nFor the repo name, use getting-started. Make sure the Visibility is Public.\nClick the Create button!\nIf you look on the right-side of the page, you\u0026rsquo;ll see a section named Docker commands. This gives an example command that you will need to run to push to this repo.\nPushing our Image In the command line, try running the push command you see on Docker Hub. Note that your command will be using your namespace, not \u0026ldquo;docker\u0026rdquo;.\n$ docker push docker/getting-started The push refers to repository [docker.io/docker/getting-started] An image does not exist locally with the tag: docker/getting-started Why did it fail? The push command was looking for an image named docker/getting-started, but didn\u0026rsquo;t find one. If you run docker image ls, you won\u0026rsquo;t see one either.\nTo fix this, we need to \u0026ldquo;tag\u0026rdquo; our existing image we\u0026rsquo;ve built to give it another name.\nLogin to the Docker Hub using the command docker login -u YOUR-USER-NAME.\nUse the docker tag command to give the getting-started image a new name. Be sure to swap out YOUR-USER-NAME with your Docker ID.\ndocker tag getting-started YOUR-USER-NAME/getting-started Now try your push command again. If you\u0026rsquo;re copying the value from Docker Hub, you can drop the tagname portion, as we didn\u0026rsquo;t add a tag to the image name. If you don\u0026rsquo;t specify a tag, Docker will use a tag called latest.\ndocker push YOUR-USER-NAME/getting-started Running our Image on a New Instance Now that our image has been built and pushed into a registry, let\u0026rsquo;s try running our app on a brand new instance that has never seen this container image! To do this, we will use Play with Docker.\nOpen your browser to Play with Docker.\nLog in with your Docker Hub account.\nOnce you\u0026rsquo;re logged in, click on the \u0026ldquo;+ ADD NEW INSTANCE\u0026rdquo; link in the left side bar. (If you don\u0026rsquo;t see it, make your browser a little wider.) After a few seconds, a terminal window will be opened in your browser.\nIn the terminal, start your freshly pushed app.\ndocker run -dp 3000:3000 YOUR-USER-NAME/getting-started You should see the image get pulled down and eventually start up!\nClick on the 3000 badge when it comes up and you should see the app with your modifications! Hooray! If the 3000 badge doesn\u0026rsquo;t show up, you can click on the \u0026ldquo;Open Port\u0026rdquo; button and type in 3000.\nRecap\u0026ndash; In this section, we learned how to share our images by pushing them to a registry. We then went to a brand new instance and were able to run the freshly pushed image. This is quite common in CI pipelines, where the pipeline will create the image and push it to a registry and then the production environment can use the latest version of the image.\nNow that we have that figured out, let\u0026rsquo;s circle back around to what we noticed at the end of the last section. As a reminder, we noticed that when we restarted the app, we lost all of our todo list items. That\u0026rsquo;s obviously not a great user experience, so let\u0026rsquo;s learn how we can persist the data across restarts!\nPersisting our DB In case you didn\u0026rsquo;t notice, our todo list is being wiped clean every single time we launch the container. Why is this? Let\u0026rsquo;s dive into how the container is working.\nThe Container\u0026rsquo;s Filesystem When a container runs, it uses the various layers from an image for its filesystem. Each container also gets its own \u0026ldquo;scratch space\u0026rdquo; to create/update/remove files. Any changes won\u0026rsquo;t be seen in another container, even if they are using the same image.\nSeeing this in Practice To see this in action, we\u0026rsquo;re going to start two containers and create a file in each. What you\u0026rsquo;ll see is that the files created in one container aren\u0026rsquo;t available in another.\nStart a ubuntu container that will create a file named /data.txt with a random number between 1 and 10000.\ndocker run -d ubuntu bash -c \u0026#34;shuf -i 1-10000 -n 1 -o /data.txt \u0026amp;\u0026amp; tail -f /dev/null\u0026#34; In case you\u0026rsquo;re curious about the command, we\u0026rsquo;re starting a bash shell and invoking two commands (why we have the \u0026amp;\u0026amp;). The first portion picks a single random number and writes it to /data.txt. The second command is simply watching a file to keep the container running.\nValidate we can see the output by exec\u0026lsquo;ing into the container. To do so, open the Dashboard and click the first action of the container that is running the ubuntu image.\nYou will see a terminal that is running a shell in the ubuntu container. Run the following command to see the content of the /data.txt file. Close this terminal afterwards again.\ncat /data.txt If you prefer the command line you can use the docker exec command to do the same. You need to get the container\u0026rsquo;s ID (use docker ps to get it) and get the content with the following command.\ndocker exec \u0026lt;container-id\u0026gt; cat /data.txt You should see a random number!\nNow, let\u0026rsquo;s start another ubuntu container (the same image) and we\u0026rsquo;ll see we don\u0026rsquo;t have the same file.\ndocker run -it ubuntu ls / And look! There\u0026rsquo;s no data.txt file there! That\u0026rsquo;s because it was written to the scratch space for only the first container.\nGo ahead and remove the first container using the docker rm -f command.\nContainer Volumes With the previous experiment, we saw that each container starts from the image definition each time it starts. While containers can create, update, and delete files, those changes are lost when the container is removed and all changes are isolated to that container. With volumes, we can change all of this.\nVolumes provide the ability to connect specific filesystem paths of the container back to the host machine. If a directory in the container is mounted, changes in that directory are also seen on the host machine. If we mount that same directory across container restarts, we\u0026rsquo;d see the same files.\nThere are two main types of volumes. We will eventually use both, but we will start with named volumes.\nPersisting our Todo Data By default, the todo app stores its data in a SQLite Database at /etc/todos/todo.db. If you\u0026rsquo;re not familiar with SQLite, no worries! It\u0026rsquo;s simply a relational database in which all of the data is stored in a single file. While this isn\u0026rsquo;t the best for large-scale applications, it works for small demos. We\u0026rsquo;ll talk about switching this to a different database engine later.\nWith the database being a single file, if we can persist that file on the host and make it available to the next container, it should be able to pick up where the last one left off. By creating a volume and attaching (often called \u0026ldquo;mounting\u0026rdquo;) it to the directory the data is stored in, we can persist the data. As our container writes to the todo.db file, it will be persisted to the host in the volume.\nAs mentioned, we are going to use a named volume. Think of a named volume as simply a bucket of data. Docker maintains the physical location on the disk and you only need to remember the name of the volume. Every time you use the volume, Docker will make sure the correct data is provided.\nCreate a volume by using the docker volume create command.\ndocker volume create todo-db Stop the todo app container once again in the Dashboard (or with docker rm -f \u0026lt;id\u0026gt;), as it is still running without using the persistent volume.\nStart the todo app container, but add the -v flag to specify a volume mount. We will use the named volume and mount it to /etc/todos, which will capture all files created at the path.\ndocker run -dp 3000:3000 -v todo-db:/etc/todos getting-started Once the container starts up, open the app and add a few items to your todo list.\nRemove the container for the todo app. Use the Dashboard or docker ps to get the ID and then docker rm -f \u0026lt;id\u0026gt; to remove it.\nStart a new container using the same command from above.\nOpen the app. You should see your items still in your list!\nGo ahead and remove the container when you\u0026rsquo;re done checking out your list.\nHooray! You\u0026rsquo;ve now learned how to persist data!\nWhile named volumes and bind mounts (which we\u0026rsquo;ll talk about in a minute) are the two main types of volumes supported by a default Docker engine installation, there are many volume driver plugins available to support NFS, SFTP, NetApp, and more! This will be especially important once you start running containers on multiple hosts in a clustered environment with Swarm, Kubernetes, etc.\nDiving into our Volume A lot of people frequently ask \u0026ldquo;Where is Docker actually storing my data when I use a named volume?\u0026rdquo; If you want to know, you can use the docker volume inspect command.\ndocker volume inspect todo-db [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;2019-09-26T02:18:36Z\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Labels\u0026#34;: {}, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/docker/volumes/todo-db/_data\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;todo-db\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] The Mountpoint is the actual location on the disk where the data is stored. Note that on most machines, you will need to have root access to access this directory from the host. But, that\u0026rsquo;s where it is!\nWhile running in Docker Desktop, the Docker commands are actually running inside a small VM on your machine. If you wanted to look at the actual contents of the Mountpoint directory, you would need to first get inside of the VM.\nRecap+ At this point, we have a functioning application that can survive restarts! We can show it off to our investors and hope they can catch our vision!\nHowever, we saw earlier that rebuilding images for every change takes quite a bit of time. There\u0026rsquo;s got to be a better way to make changes, right? With bind mounts (which we hinted at earlier), there is a better way! Let\u0026rsquo;s take a look at that now!\nUsing Bind Mounts In the previous chapter, we talked about and used a named volume to persist the data in our database. Named volumes are great if we simply want to store data, as we don\u0026rsquo;t have to worry about where the data is stored.\nWith bind mounts, we control the exact mountpoint on the host. We can use this to persist data, but is often used to provide additional data into containers. When working on an application, we can use a bind mount to mount our source code into the container to let it see code changes, respond, and let us see the changes right away.\nFor Node-based applications, nodemon is a great tool to watch for file changes and then restart the application. There are equivalent tools in most other languages and frameworks.\nQuick Volume Type Comparisons Bind mounts and named volumes are the two main types of volumes that come with the Docker engine. However, additional volume drivers are available to support other use cases (SFTP, Ceph, NetApp, S3, and more).\nNamed Volumes Bind Mounts Host Location Docker chooses You control Mount Example (using -v) my-volume:/usr/local/data /path/to/data:/usr/local/data Populates new volume with container contents Yes No Supports Volume Drivers Yes No Starting a Dev-Mode Container To run our container to support a development workflow, we will do the following:\nMount our source code into the container Install all dependencies, including the \u0026ldquo;dev\u0026rdquo; dependencies Start nodemon to watch for filesystem changes So, let\u0026rsquo;s do it!\nMake sure you don\u0026rsquo;t have any previous getting-started containers running.\nRun the following command. We\u0026rsquo;ll explain what\u0026rsquo;s going on afterwards:\ndocker run -dp 3000:3000 \\ -w /app -v \u0026#34;$(pwd):/app\u0026#34; \\ node:12-alpine \\ sh -c \u0026#34;yarn install \u0026amp;\u0026amp; yarn run dev\u0026#34; If you are using PowerShell then use this command.\ndocker run -dp 3000:3000 ` -w /app -v \u0026#34;$(pwd):/app\u0026#34; ` node:12-alpine ` sh -c \u0026#34;yarn install \u0026amp;\u0026amp; yarn run dev\u0026#34; -dp 3000:3000 - same as before. Run in detached (background) mode and create a port mapping -w /app - sets the \u0026ldquo;working directory\u0026rdquo; or the current directory that the command will run from -v \u0026quot;$(pwd):/app\u0026quot; - bind mount the current directory from the host in the container into the /app directory node:12-alpine - the image to use. Note that this is the base image for our app from the Dockerfile sh -c \u0026quot;yarn install \u0026amp;\u0026amp; yarn run dev\u0026quot; - the command. We\u0026rsquo;re starting a shell using sh (alpine doesn\u0026rsquo;t have bash) and running yarn install to install all dependencies and then running yarn run dev. If we look in the package.json, we\u0026rsquo;ll see that the dev script is starting nodemon. You can watch the logs using docker logs -f \u0026lt;container-id\u0026gt;. You\u0026rsquo;ll know you\u0026rsquo;re ready to go when you see this\u0026hellip;\ndocker logs -f \u0026lt;container-id\u0026gt; $ nodemon src/index.js [nodemon] 1.19.2 [nodemon] to restart at any time, enter `rs` [nodemon] watching dir(s): *.* [nodemon] starting `node src/index.js` Using sqlite database at /etc/todos/todo.db Listening on port 3000 When you\u0026rsquo;re done watching the logs, exit out by hitting Ctrl+C.\nNow, let\u0026rsquo;s make a change to the app. In the src/static/js/app.js file, let\u0026rsquo;s change the \u0026ldquo;Add Item\u0026rdquo; button to simply say \u0026ldquo;Add\u0026rdquo;. This change will be on line 109.\n- {submitting ? \u0026#39;Adding...\u0026#39; : \u0026#39;Add Item\u0026#39;} + {submitting ? \u0026#39;Adding...\u0026#39; : \u0026#39;Add\u0026#39;} Simply refresh the page (or open it) and you should see the change reflected in the browser almost immediately. It might take a few seconds for the Node server to restart, so if you get an error, just try refreshing after a few seconds.\nFeel free to make any other changes you\u0026rsquo;d like to make. When you\u0026rsquo;re done, stop the container and build your new image using docker build -t getting-started ..\nUsing bind mounts is very common for local development setups. The advantage is that the dev machine doesn\u0026rsquo;t need to have all of the build tools and environments installed. With a single docker run command, the dev environment is pulled and ready to go. We\u0026rsquo;ll talk about Docker Compose in a future step, as this will help simplify our commands (we\u0026rsquo;re already getting a lot of flags).\nRecap= At this point, we can persist our database and respond rapidly to the needs and demands of our investors and founders. Hooray! But, guess what? We received great news!\nYour project has been selected for future development!\nIn order to prepare for production, we need to migrate our database from working in SQLite to something that can scale a little better. For simplicity, we\u0026rsquo;ll keep with a relational database and switch our application to use MySQL. But, how should we run MySQL? How do we allow the containers to talk to each other? We\u0026rsquo;ll talk about that next!\nMulti-Container Apps Up to this point, we have been working with single container apps. But, we now want to add MySQL to the application stack. The following question often arises - \u0026ldquo;Where will MySQL run? Install it in the same container or run it separately?\u0026rdquo; In general, each container should do one thing and do it well. A few reasons:\nThere\u0026rsquo;s a good chance you\u0026rsquo;d have to scale APIs and front-ends differently than databases Separate containers let you version and update versions in isolation While you may use a container for the database locally, you may want to use a managed service for the database in production. You don\u0026rsquo;t want to ship your database engine with your app then. Running multiple processes will require a process manager (the container only starts one process), which adds complexity to container startup/shutdown And there are more reasons. So, we will update our application to work like this:\nContainer Networking Remember that containers, by default, run in isolation and don\u0026rsquo;t know anything about other processes or containers on the same machine. So, how do we allow one container to talk to another? The answer is networking. Now, you don\u0026rsquo;t have to be a network engineer (hooray!). Simply remember this rule\u0026hellip;\nIf two containers are on the same network, they can talk to each other. If they aren\u0026rsquo;t, they can\u0026rsquo;t.\nStarting MySQL There are two ways to put a container on a network: 1) Assign it at start or 2) connect an existing container. For now, we will create the network first and attach the MySQL container at startup.\nCreate the network.\ndocker network create todo-app Start a MySQL container and attach it to the network. We\u0026rsquo;re also going to define a few environment variables that the database will use to initialize the database (see the \u0026ldquo;Environment Variables\u0026rdquo; section in the MySQL Docker Hub listing).\ndocker run -d \\ --network todo-app --network-alias mysql \\ -v todo-mysql-data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=secret \\ -e MYSQL_DATABASE=todos \\ mysql:5.7 If you are using PowerShell then use this command.\ndocker run -d ` --network todo-app --network-alias mysql ` -v todo-mysql-data:/var/lib/mysql ` -e MYSQL_ROOT_PASSWORD=secret ` -e MYSQL_DATABASE=todos ` mysql:5.7 You\u0026rsquo;ll also see we specified the --network-alias flag. We\u0026rsquo;ll come back to that in just a moment.\nYou\u0026rsquo;ll notice we\u0026rsquo;re using a volume named todo-mysql-data here and mounting it at /var/lib/mysql, which is where MySQL stores its data. However, we never ran a docker volume create command. Docker recognizes we want to use a named volume and creates one automatically for us.\nTo confirm we have the database up and running, connect to the database and verify it connects.\ndocker exec -it \u0026lt;mysql-container-id\u0026gt; mysql -p When the password prompt comes up, type in secret. In the MySQL shell, list the databases and verify you see the todos database.\nmysql\u0026gt; SHOW DATABASES; You should see output that looks like this:\n+--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | todos | +--------------------+ 5 rows in set (0.00 sec) Hooray! We have our todos database and it\u0026rsquo;s ready for us to use!\nConnecting to MySQL Now that we know MySQL is up and running, let\u0026rsquo;s use it! But, the question is\u0026hellip; how? If we run another container on the same network, how do we find the container (remember each container has its own IP address)?\nTo figure it out, we\u0026rsquo;re going to make use of the nicolaka/netshoot container, which ships with a lot of tools that are useful for troubleshooting or debugging networking issues.\nStart a new container using the nicolaka/netshoot image. Make sure to connect it to the same network.\ndocker run -it --network todo-app nicolaka/netshoot Inside the container, we\u0026rsquo;re going to use the dig command, which is a useful DNS tool. We\u0026rsquo;re going to look up the IP address for the hostname mysql.\ndig mysql And you\u0026rsquo;ll get an output like this\u0026hellip;\n; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.14.1 \u0026lt;\u0026lt;\u0026gt;\u0026gt; mysql ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 32162 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;mysql. IN A ;; ANSWER SECTION: mysql. 600 IN A 172.23.0.2 ;; Query time: 0 msec ;; SERVER: 127.0.0.11#53(127.0.0.11) ;; WHEN: Tue Oct 01 23:47:24 UTC 2019 ;; MSG SIZE rcvd: 44 In the \u0026ldquo;ANSWER SECTION\u0026rdquo;, you will see an A record for mysql that resolves to 172.23.0.2 (your IP address will most likely have a different value). While mysql isn\u0026rsquo;t normally a valid hostname, Docker was able to resolve it to the IP address of the container that had that network alias (remember the --network-alias flag we used earlier?).\nWhat this means is\u0026hellip; our app only simply needs to connect to a host named mysql and it\u0026rsquo;ll talk to the database! It doesn\u0026rsquo;t get much simpler than that!\nRunning our App with MySQL The todo app supports the setting of a few environment variables to specify MySQL connection settings. They are:\nMYSQL_HOST - the hostname for the running MySQL server MYSQL_USER - the username to use for the connection MYSQL_PASSWORD - the password to use for the connection MYSQL_DB - the database to use once connected While using env vars to set connection settings is generally ok for development, it is HIGHLY DISCOURAGED when running applications in production. Diogo Monica, the former lead of security at Docker, wrote a fantastic blog post explaining why.\nA more secure mechanism is to use the secret support provided by your container orchestration framework. In most cases, these secrets are mounted as files in the running container. You\u0026rsquo;ll see many apps (including the MySQL image and the todo app) also support env vars with a _FILE suffix to point to a file containing the variable.\nAs an example, setting the MYSQL_PASSWORD_FILE var will cause the app to use the contents of the referenced file as the connection password. Docker doesn\u0026rsquo;t do anything to support these env vars. Your app will need to know to look for the variable and get the file contents.\nWith all of that explained, let\u0026rsquo;s start our dev-ready container!\nWe\u0026rsquo;ll specify each of the environment variables above, as well as connect the container to our app network.\ndocker run -dp 3000:3000 \\ -w /app -v \u0026#34;$(pwd):/app\u0026#34; \\ --network todo-app \\ -e MYSQL_HOST=mysql \\ -e MYSQL_USER=root \\ -e MYSQL_PASSWORD=secret \\ -e MYSQL_DB=todos \\ node:12-alpine \\ sh -c \u0026#34;yarn install \u0026amp;\u0026amp; yarn run dev\u0026#34; If you are using PowerShell then use this command.\ndocker run -dp 3000:3000 ` -w /app -v \u0026#34;$(pwd):/app\u0026#34; ` --network todo-app ` -e MYSQL_HOST=mysql ` -e MYSQL_USER=root ` -e MYSQL_PASSWORD=secret ` -e MYSQL_DB=todos ` node:12-alpine ` sh -c \u0026#34;yarn install \u0026amp;\u0026amp; yarn run dev\u0026#34; If we look at the logs for the container (docker logs \u0026lt;container-id\u0026gt;), we should see a message indicating it\u0026rsquo;s using the mysql database.\n# Previous log messages omitted $ nodemon src/index.js [nodemon] 1.19.2 [nodemon] to restart at any time, enter `rs` [nodemon] watching dir(s): *.* [nodemon] starting `node src/index.js` Connected to mysql db at host mysql Listening on port 3000 Open the app in your browser and add a few items to your todo list.\nConnect to the mysql database and prove that the items are being written to the database. Remember, the password is secret.\ndocker exec -it \u0026lt;mysql-container-id\u0026gt; mysql -p todos And in the mysql shell, run the following:\nmysql\u0026gt; select * from todo_items; +--------------------------------------+--------------------+-----------+ | id | name | completed | +--------------------------------------+--------------------+-----------+ | c906ff08-60e6-44e6-8f49-ed56a0853e85 | Do amazing things! | 0 | | 2912a79e-8486-4bc3-a4c5-460793a575ab | Be awesome! | 0 | +--------------------------------------+--------------------+-----------+ Obviously, your table will look different because it has your items. But, you should see them stored there!\nIf you take a quick look at the Docker Dashboard, you\u0026rsquo;ll see that we have two app containers running. But, there\u0026rsquo;s no real indication that they are grouped together in a single app. We\u0026rsquo;ll see how to make that better shortly!\nRecap_ At this point, we have an application that now stores its data in an external database running in a separate container. We learned a little bit about container networking and saw how service discovery can be performed using DNS.\nBut, there\u0026rsquo;s a good chance you are starting to feel a little overwhelmed with everything you need to do to start up this application. We have to create a network, start containers, specify all of the environment variables, expose ports, and more! That\u0026rsquo;s a lot to remember and it\u0026rsquo;s certainly making things harder to pass along to someone else.\nIn the next section, we\u0026rsquo;ll talk about Docker Compose. With Docker Compose, we can share our application stacks in a much easier way and let others spin them up with a single (and simple) command!\nUsing Docker Compose Docker Compose is a tool that was developed to help define and share multi-container applications. With Compose, we can create a YAML file to define the services and with a single command, can spin everything up or tear it all down.\nThe big advantage of using Compose is you can define your application stack in a file, keep it at the root of your project repo (it\u0026rsquo;s now version controlled), and easily enable someone else to contribute to your project. Someone would only need to clone your repo and start the compose app. In fact, you might see quite a few projects on GitHub/GitLab doing exactly this now.\nSo, how do we get started?\nInstalling Docker Compose If you installed Docker Desktop/Toolbox for either Windows or Mac, you already have Docker Compose! Play-with-Docker instances already have Docker Compose installed as well. If you are on a Linux machine, you will need to install Docker Compose using the instructions here.\nAfter installation, you should be able to run the following and see version information.\ndocker-compose version Creating our Compose File At the root of the app project, create a file named docker-compose.yml.\nIn the compose file, we\u0026rsquo;ll start off by defining the schema version. In most cases, it\u0026rsquo;s best to use the latest supported version. You can look at the Compose file reference for the current schema versions and the compatibility matrix.\nversion: \u0026#39;3.7\u0026#39; Next, we\u0026rsquo;ll define the list of services (or containers) we want to run as part of our application.\nversion: \u0026#39;3.7\u0026#39; services: And now, we\u0026rsquo;ll start migrating a service at a time into the compose file.\nDefining the App Service To remember, this was the command we were using to define our app container.\ndocker run -dp 3000:3000 \\ -w /app -v \u0026#34;$(pwd):/app\u0026#34; \\ --network todo-app \\ -e MYSQL_HOST=mysql \\ -e MYSQL_USER=root \\ -e MYSQL_PASSWORD=secret \\ -e MYSQL_DB=todos \\ node:12-alpine \\ sh -c \u0026#34;yarn install \u0026amp;\u0026amp; yarn run dev\u0026#34; If you are using PowerShell then use this command.\ndocker run -dp 3000:3000 ` -w /app -v \u0026#34;$(pwd):/app\u0026#34; ` --network todo-app ` -e MYSQL_HOST=mysql ` -e MYSQL_USER=root ` -e MYSQL_PASSWORD=secret ` -e MYSQL_DB=todos ` node:12-alpine ` sh -c \u0026#34;yarn install \u0026amp;\u0026amp; yarn run dev\u0026#34; First, let\u0026rsquo;s define the service entry and the image for the container. We can pick any name for the service. The name will automatically become a network alias, which will be useful when defining our MySQL service.\nversion: \u0026#39;3.7\u0026#39; services: app: image: node:12-alpine Typically, you will see the command close to the image definition, although there is no requirement on ordering. So, let\u0026rsquo;s go ahead and move that into our file.\nversion: \u0026#39;3.7\u0026#39; services: app: image: node:12-alpine command: sh -c \u0026#34;yarn install \u0026amp;\u0026amp; yarn run dev\u0026#34; Let\u0026rsquo;s migrate the -p 3000:3000 part of the command by defining the ports for the service. We will use the short syntax here, but there is also a more verbose long syntax available as well.\nversion: \u0026#39;3.7\u0026#39; services: app: image: node:12-alpine command: sh -c \u0026#34;yarn install \u0026amp;\u0026amp; yarn run dev\u0026#34; ports: - 3000:3000 Next, we\u0026rsquo;ll migrate both the working directory (-w /app) and the volume mapping (-v \u0026quot;$(pwd):/app\u0026quot;) by using the working_dir and volumes definitions. Volumes also has a short and long syntax.\nOne advantage of Docker Compose volume definitions is we can use relative paths from the current directory.\nversion: \u0026#39;3.7\u0026#39; services: app: image: node:12-alpine command: sh -c \u0026#34;yarn install \u0026amp;\u0026amp; yarn run dev\u0026#34; ports: - 3000:3000 working_dir: /app volumes: - ./:/app Finally, we need to migrate the environment variable definitions using the environment key.\nversion: \u0026#39;3.7\u0026#39; services: app: image: node:12-alpine command: sh -c \u0026#34;yarn install \u0026amp;\u0026amp; yarn run dev\u0026#34; ports: - 3000:3000 working_dir: /app volumes: - ./:/app environment: MYSQL_HOST: mysql MYSQL_USER: root MYSQL_PASSWORD: secret MYSQL_DB: todos Defining the MySQL Service Now, it\u0026rsquo;s time to define the MySQL service. The command that we used for that container was the following:\ndocker run -d \\ --network todo-app --network-alias mysql \\ -v todo-mysql-data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=secret \\ -e MYSQL_DATABASE=todos \\ mysql:5.7 If you are using PowerShell then use this command.\ndocker run -d ` --network todo-app --network-alias mysql ` -v todo-mysql-data:/var/lib/mysql ` -e MYSQL_ROOT_PASSWORD=secret ` -e MYSQL_DATABASE=todos ` mysql:5.7 We will first define the new service and name it mysql so it automatically gets the network alias. We\u0026rsquo;ll go ahead and specify the image to use as well.\nversion: \u0026#39;3.7\u0026#39; services: app: # The app service definition mysql: image: mysql:5.7 Next, we\u0026rsquo;ll define the volume mapping. When we ran the container with docker run, the named volume was created automatically. However, that doesn\u0026rsquo;t happen when running with Compose. We need to define the volume in the top-level volumes: section and then specify the mountpoint in the service config. By simply providing only the volume name, the default options are used. There are many more options available though.\nversion: \u0026#39;3.7\u0026#39; services: app: # The app service definition mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql volumes: todo-mysql-data: Finally, we only need to specify the environment variables.\nversion: \u0026#39;3.7\u0026#39; services: app: # The app service definition mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: todos volumes: todo-mysql-data: At this point, our complete docker-compose.yml should look like this:\nversion: \u0026#39;3.7\u0026#39; services: app: image: node:12-alpine command: sh -c \u0026#34;yarn install \u0026amp;\u0026amp; yarn run dev\u0026#34; ports: - 3000:3000 working_dir: /app volumes: - ./:/app environment: MYSQL_HOST: mysql MYSQL_USER: root MYSQL_PASSWORD: secret MYSQL_DB: todos mysql: image: mysql:5.7 volumes: - todo-mysql-data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: todos volumes: todo-mysql-data: Running our Application Stack Now that we have our docker-compose.yml file, we can start it up!\nMake sure no other copies of the app/db are running first (docker ps and docker rm -f \u0026lt;ids\u0026gt;).\nStart up the application stack using the docker-compose up command. We\u0026rsquo;ll add the -d flag to run everything in the background.\ndocker-compose up -d When we run this, we should see output like this:\nCreating network \u0026#34;app_default\u0026#34; with the default driver Creating volume \u0026#34;app_todo-mysql-data\u0026#34; with default driver Creating app_app_1 ... done Creating app_mysql_1 ... done You\u0026rsquo;ll notice that the volume was created as well as a network! By default, Docker Compose automatically creates a network specifically for the application stack (which is why we didn\u0026rsquo;t define one in the compose file).\nLet\u0026rsquo;s look at the logs using the docker-compose logs -f command. You\u0026rsquo;ll see the logs from each of the services interleaved into a single stream. This is incredibly useful when you want to watch for timing-related issues. The -f flag \u0026ldquo;follows\u0026rdquo; the log, so will give you live output as it\u0026rsquo;s generated.\nIf you don\u0026rsquo;t already, you\u0026rsquo;ll see output that looks like this\u0026hellip;\nmysql_1 | 2019-10-03T03:07:16.083639Z 0 [Note] mysqld: ready for connections. mysql_1 | Version: \u0026#39;5.7.27\u0026#39; socket: \u0026#39;/var/run/mysqld/mysqld.sock\u0026#39; port: 3306 MySQL Community Server (GPL) app_1 | Connected to mysql db at host mysql app_1 | Listening on port 3000 The service name is displayed at the beginning of the line (often colored) to help distinguish messages. If you want to view the logs for a specific service, you can add the service name to the end of the logs command (for example, docker-compose logs -f app).\nWhen the app is starting up, it actually sits and waits for MySQL to be up and ready before trying to connect to it. Docker doesn\u0026rsquo;t have any built-in support to wait for another container to be fully up, running, and ready before starting another container. For Node-based projects, you can use the wait-port dependency. Similar projects exist for other languages/frameworks.\nAt this point, you should be able to open your app and see it running. And hey! We\u0026rsquo;re down to a single command!\nSeeing our App Stack in Docker Dashboard If we look at the Docker Dashboard, we\u0026rsquo;ll see that there is a group named app. This is the \u0026ldquo;project name\u0026rdquo; from Docker Compose and used to group the containers together. By default, the project name is simply the name of the directory that the docker-compose.yml was located in.\nIf you twirl down the app, you will see the two containers we defined in the compose file. The names are also a little more descriptive, as they follow the pattern of \u0026lt;project-name\u0026gt;_\u0026lt;service-name\u0026gt;_\u0026lt;replica-number\u0026gt;. So, it\u0026rsquo;s very easy to quickly see what container is our app and which container is the mysql database.\nTearing it All Down When you\u0026rsquo;re ready to tear it all down, simply run docker-compose down or hit the trash can on the Docker Dashboard for the entire app. The containers will stop and the network will be removed.\nBy default, named volumes in your compose file are NOT removed when running docker-compose down. If you want to remove the volumes, you will need to add the --volumes flag.\nThe Docker Dashboard does not remove volumes when you delete the app stack.\nOnce torn down, you can switch to another project, run docker-compose up and be ready to contribute to that project! It really doesn\u0026rsquo;t get much simpler than that!\nRecap\\ In this section, we learned about Docker Compose and how it helps us dramatically simplify the defining and sharing of multi-service applications. We created a Compose file by translating the commands we were using into the appropriate compose format.\nAt this point, we\u0026rsquo;re starting to wrap up the tutorial. However, there are a few best practices about image building we want to cover, as there is a big issue with the Dockerfile we\u0026rsquo;ve been using. So, let\u0026rsquo;s take a look!\nImage Building Best Practices Security Scanning When you have built an image, it is good practice to scan it for security vulnerabilities using the docker scan command. Docker has partnered with Snyk to provide the vulnerability scanning service.\nFor example, to scan the getting-started image you created earlier in the tutorial, you can just type\ndocker scan getting-started The scan uses a constantly updated database of vulnerabilities, so the output you see will vary as new vulnerabilities are discovered, but it might look something like this:\n✗ Low severity vulnerability found in freetype/freetype Description: CVE-2020-15999 Info: https://snyk.io/vuln/SNYK-ALPINE310-FREETYPE-1019641 Introduced through: freetype/freetype@2.10.0-r0, gd/libgd@2.2.5-r2 From: freetype/freetype@2.10.0-r0 From: gd/libgd@2.2.5-r2 \u0026gt; freetype/freetype@2.10.0-r0 Fixed in: 2.10.0-r1 ✗ Medium severity vulnerability found in libxml2/libxml2 Description: Out-of-bounds Read Info: https://snyk.io/vuln/SNYK-ALPINE310-LIBXML2-674791 Introduced through: libxml2/libxml2@2.9.9-r3, libxslt/libxslt@1.1.33-r3, nginx-module-xslt/nginx-module-xslt@1.17.9-r1 From: libxml2/libxml2@2.9.9-r3 From: libxslt/libxslt@1.1.33-r3 \u0026gt; libxml2/libxml2@2.9.9-r3 From: nginx-module-xslt/nginx-module-xslt@1.17.9-r1 \u0026gt; libxml2/libxml2@2.9.9-r3 Fixed in: 2.9.9-r4 The output lists the type of vulnerability, a URL to learn more, and importantly which version of the relevant library fixes the vulnerability.\nThere are several other options, which you can read about in the docker scan documentation.\nAs well as scanning your newly built image on the command line, you can also configure Docker Hub to scan all newly pushed images automatically, and you can then see the results in both Docker Hub and Docker Desktop.\nImage Layering Did you know that you can look at what makes up an image? Using the docker image history command, you can see the command that was used to create each layer within an image.\nUse the docker image history command to see the layers in the getting-started image you created earlier in the tutorial.\ndocker image history getting-started You should get output that looks something like this (dates/IDs may be different).\nIMAGE CREATED CREATED BY SIZE COMMENT a78a40cbf866 18 seconds ago /bin/sh -c #(nop) CMD [\u0026#34;node\u0026#34; \u0026#34;src/index.j… 0B f1d1808565d6 19 seconds ago /bin/sh -c yarn install --production 85.4MB a2c054d14948 36 seconds ago /bin/sh -c #(nop) COPY dir:5dc710ad87c789593… 198kB 9577ae713121 37 seconds ago /bin/sh -c #(nop) WORKDIR /app 0B b95baba1cfdb 13 days ago /bin/sh -c #(nop) CMD [\u0026#34;node\u0026#34;] 0B \u0026lt;missing\u0026gt; 13 days ago /bin/sh -c #(nop) ENTRYPOINT [\u0026#34;docker-entry… 0B \u0026lt;missing\u0026gt; 13 days ago /bin/sh -c #(nop) COPY file:238737301d473041… 116B \u0026lt;missing\u0026gt; 13 days ago /bin/sh -c apk add --no-cache --virtual .bui… 5.35MB \u0026lt;missing\u0026gt; 13 days ago /bin/sh -c #(nop) ENV YARN_VERSION=1.21.1 0B \u0026lt;missing\u0026gt; 13 days ago /bin/sh -c addgroup -g 1000 node \u0026amp;\u0026amp; addu… 74.3MB \u0026lt;missing\u0026gt; 13 days ago /bin/sh -c #(nop) ENV NODE_VERSION=12.14.1 0B \u0026lt;missing\u0026gt; 13 days ago /bin/sh -c #(nop) CMD [\u0026#34;/bin/sh\u0026#34;] 0B \u0026lt;missing\u0026gt; 13 days ago /bin/sh -c #(nop) ADD file:e69d441d729412d24… 5.59MB Each of the lines represents a layer in the image. The display here shows the base at the bottom with the newest layer at the top. Using this, you can also quickly see the size of each layer, helping diagnose large images.\nYou\u0026rsquo;ll notice that several of the lines are truncated. If you add the --no-trunc flag, you\u0026rsquo;ll get the full output (yes\u0026hellip; funny how you use a truncated flag to get untruncated output, huh?)\ndocker image history --no-trunc getting-started Layer Caching Now that you\u0026rsquo;ve seen the layering in action, there\u0026rsquo;s an important lesson to learn to help decrease build times for your container images.\nOnce a layer changes, all downstream layers have to be recreated as well\nLet\u0026rsquo;s look at the Dockerfile we were using one more time\u0026hellip;\nFROM node:12-alpine WORKDIR /app COPY . . RUN yarn install --production CMD [\u0026#34;node\u0026#34;, \u0026#34;src/index.js\u0026#34;] Going back to the image history output, we see that each command in the Dockerfile becomes a new layer in the image. You might remember that when we made a change to the image, the yarn dependencies had to be reinstalled. Is there a way to fix this? It doesn\u0026rsquo;t make much sense to ship around the same dependencies every time we build, right?\nTo fix this, we need to restructure our Dockerfile to help support the caching of the dependencies. For Node-based applications, those dependencies are defined in the package.json file. So, what if we copied only that file in first, install the dependencies, and then copy in everything else? Then, we only recreate the yarn dependencies if there was a change to the package.json. Make sense?\nUpdate the Dockerfile to copy in the package.json first, install dependencies, and then copy everything else in.\nFROM node:12-alpine WORKDIR /app COPY package.json yarn.lock ./ RUN yarn install --production COPY . . CMD [\u0026#34;node\u0026#34;, \u0026#34;src/index.js\u0026#34;] Create a file named .dockerignore in the same folder as the Dockerfile with the following contents.\nnode_modules .dockerignore files are an easy way to selectively copy only image relevant files. You can read more about this here. In this case, the node_modules folder should be omitted in the second COPY step because otherwise, it would possibly overwrite files which were created by the command in the RUN step. For further details on why this is recommended for Node.js applications and other best practices, have a look at their guide on Dockerizing a Node.js web app.\nBuild a new image using docker build.\ndocker build -t getting-started . You should see output like this\u0026hellip;\nSending build context to Docker daemon 219.1kB Step 1/6 : FROM node:12-alpine ---\u0026gt; b0dc3a5e5e9e Step 2/6 : WORKDIR /app ---\u0026gt; Using cache ---\u0026gt; 9577ae713121 Step 3/6 : COPY package.json yarn.lock ./ ---\u0026gt; bd5306f49fc8 Step 4/6 : RUN yarn install --production ---\u0026gt; Running in d53a06c9e4c2 yarn install v1.17.3 [1/4] Resolving packages... [2/4] Fetching packages... info fsevents@1.2.9: The platform \u0026#34;linux\u0026#34; is incompatible with this module. info \u0026#34;fsevents@1.2.9\u0026#34; is an optional dependency and failed compatibility check. Excluding it from installation. [3/4] Linking dependencies... [4/4] Building fresh packages... Done in 10.89s. Removing intermediate container d53a06c9e4c2 ---\u0026gt; 4e68fbc2d704 Step 5/6 : COPY . . ---\u0026gt; a239a11f68d8 Step 6/6 : CMD [\u0026#34;node\u0026#34;, \u0026#34;src/index.js\u0026#34;] ---\u0026gt; Running in 49999f68df8f Removing intermediate container 49999f68df8f ---\u0026gt; e709c03bc597 Successfully built e709c03bc597 Successfully tagged getting-started:latest You\u0026rsquo;ll see that all layers were rebuilt. Perfectly fine since we changed the Dockerfile quite a bit.\nNow, make a change to the src/static/index.html file (like change the \u0026lt;title\u0026gt; to say \u0026ldquo;The Awesome Todo App\u0026rdquo;).\nBuild the Docker image now using docker build -t getting-started . again. This time, your output should look a little different.\nSending build context to Docker daemon 219.1kB Step 1/6 : FROM node:12-alpine ---\u0026gt; b0dc3a5e5e9e Step 2/6 : WORKDIR /app ---\u0026gt; Using cache ---\u0026gt; 9577ae713121 Step 3/6 : COPY package.json yarn.lock ./ ---\u0026gt; Using cache ---\u0026gt; bd5306f49fc8 Step 4/6 : RUN yarn install --production ---\u0026gt; Using cache ---\u0026gt; 4e68fbc2d704 Step 5/6 : COPY . . ---\u0026gt; cccde25a3d9a Step 6/6 : CMD [\u0026#34;node\u0026#34;, \u0026#34;src/index.js\u0026#34;] ---\u0026gt; Running in 2be75662c150 Removing intermediate container 2be75662c150 ---\u0026gt; 458e5c6f080c Successfully built 458e5c6f080c Successfully tagged getting-started:latest First off, you should notice that the build was MUCH faster! And, you\u0026rsquo;ll see that steps 1-4 all have Using cache. So, hooray! We\u0026rsquo;re using the build cache. Pushing and pulling this image and updates to it will be much faster as well. Hooray!\nMulti-Stage Builds While we\u0026rsquo;re not going to dive into it too much in this tutorial, multi-stage builds are an incredibly powerful tool to help use multiple stages to create an image. There are several advantages for them:\nSeparate build-time dependencies from runtime dependencies Reduce overall image size by shipping only what your app needs to run Maven/Tomcat Example When building Java-based applications, a JDK is needed to compile the source code to Java bytecode. However, that JDK isn\u0026rsquo;t needed in production. Also, you might be using tools like Maven or Gradle to help build the app. Those also aren\u0026rsquo;t needed in our final image. Multi-stage builds help.\nFROM maven AS build WORKDIR /app COPY . . RUN mvn package FROM tomcat COPY --from=build /app/target/file.war /usr/local/tomcat/webapps In this example, we use one stage (called build) to perform the actual Java build using Maven. In the second stage (starting at FROM tomcat), we copy in files from the build stage. The final image is only the last stage being created (which can be overridden using the --target flag).\nReact Example When building React applications, we need a Node environment to compile the JS code (typically JSX), SASS stylesheets, and more into static HTML, JS, and CSS. If we aren\u0026rsquo;t doing server-side rendering, we don\u0026rsquo;t even need a Node environment for our production build. Why not ship the static resources in a static nginx container?\nFROM node:12 AS build WORKDIR /app COPY package* yarn.lock ./ RUN yarn install COPY public ./public COPY src ./src RUN yarn run build FROM nginx:alpine COPY --from=build /app/build /usr/share/nginx/html Here, we are using a node:12 image to perform the build (maximizing layer caching) and then copying the output into an nginx container. Cool, huh?\nRecap` By understanding a little bit about how images are structured, we can build images faster and ship fewer changes. Scanning images gives us confidence that the containers we are running and distributing are secure. Multi-stage builds also help us reduce overall image size and increase final container security by separating build-time dependencies from runtime dependencies.\nWhat Next? Although we\u0026rsquo;re done with our workshop, there\u0026rsquo;s still a LOT more to learn about containers! We\u0026rsquo;re not going to go deep-dive here, but here are a few other areas to look at next!\nContainer Orchestration Running containers in production is tough. You don\u0026rsquo;t want to log into a machine and simply run a docker run or docker-compose up. Why not? Well, what happens if the containers die? How do you scale across several machines? Container orchestration solves this problem. Tools like Kubernetes, Swarm, Nomad, and ECS all help solve this problem, all in slightly different ways.\nThe general idea is that you have \u0026ldquo;managers\u0026rdquo; who receive expected state. This state might be \u0026ldquo;I want to run two instances of my web app and expose port 80.\u0026rdquo; The managers then look at all of the machines in the cluster and delegate work to \u0026ldquo;worker\u0026rdquo; nodes. The managers watch for changes (such as a container quitting) and then work to make actual state reflect the expected state.\nCloud Native Computing Foundation Projects The CNCF is a vendor-neutral home for various open-source projects, including Kubernetes, Prometheus, Envoy, Linkerd, NATS, and more! You can view the graduated and incubated projects here and the entire CNCF Landscape here. There are a LOT of projects to help solve problems around monitoring, logging, security, image registries, messaging, and more!\nSo, if you\u0026rsquo;re new to the container landscape and cloud-native application development, welcome! Please connect to the community, ask questions, and keep learning! We\u0026rsquo;re excited to have you!\n","date":"2021-04-20T10:35:12Z","image":"https://static.aecra.cn/cover/docker-tutorial.jpg","permalink":"https://www.aecra.cn/en/article/docker-tutorial/","title":"Docker tutorial"},{"content":"PHP offers three different APIs to connect to MySQL. These are the mysql(removed as of PHP 7), mysqli, and PDO extensions.\nThe mysql_ functions used to be very popular, but their use is not encouraged anymore. The documentation team is discussing the database security situation, and educating users to move away from the commonly used ext/mysql extension is part of this (check php.internals: deprecating ext/mysql).\nAnd the later PHP developer team has taken the decision to generate E_DEPRECATED errors when users connect to MySQL, whether through mysql_connect(), mysql_pconnect() or the implicit connection functionality built into ext/mysql.\next/mysql was officially deprecated as of PHP 5.5 and has been removed as of PHP 7.\nSee the Red Box?\nWhen you go on any mysql_ function manual page, you see a red box, explaining it should not be used anymore.\nWhat is the shortcomings of mysql_*? Moving away from ext/mysql is not only about security, but also about having access to all the features of the MySQL database.\next/mysql was built for MySQL 3.23 and only got very few additions since then while mostly keeping compatibility with this old version which makes the code a bit harder to maintain. Missing features that is not supported by ext/mysql include: (from PHP manual).\nStored procedures (can\u0026rsquo;t handle multiple result sets) Prepared statements Encryption (SSL) Compression Full Charset support Reason to not use mysql_* function:\nNot under active development Removed as of PHP 7 Lacks an OO interface Doesn\u0026rsquo;t support non-blocking, asynchronous queries Doesn\u0026rsquo;t support prepared statements or parameterized queries Doesn\u0026rsquo;t support stored procedures Doesn\u0026rsquo;t support multiple statements Doesn\u0026rsquo;t support transactions Doesn\u0026rsquo;t support all of the functionality in MySQL 5.1 Above point quoted from Quentin\u0026rsquo;s answer\nLack of support for prepared statements is particularly important as they provide a clearer, less error prone method of escaping and quoting external data than manually escaping it with a separate function call.\nSee the comparison of SQL extensions.\nContrast use of mysql_* and PDO Q. First question in my mind was: what is PDO? A. “PDO – PHP Data Objects – is a database access layer providing a uniform method of access to multiple databases.”\nConnecting to MySQL With mysql_ function or we can say it the old way (deprecated in PHP 5.5 and above)\n$link = mysql_connect(\u0026#39;localhost\u0026#39;, \u0026#39;user\u0026#39;, \u0026#39;pass\u0026#39;); mysql_select_db(\u0026#39;testdb\u0026#39;, $link); mysql_set_charset(\u0026#39;UTF-8\u0026#39;, $link); With PDO: All you need to do is create a new PDO object. The constructor accepts parameters for specifying the database source PDO\u0026rsquo;s constructor mostly takes four parameters which are DSN (data source name) and optionally username, password.\nHere I think you are familiar with all except DSN; this is new in PDO. A DSN is basically a string of options that tell PDO which driver to use, and connection details. For further reference, check PDO MySQL DSN.\n$db = new PDO(\u0026#39;mysql:host=localhost;dbname=testdb;charset=utf8\u0026#39;, \u0026#39;username\u0026#39;, \u0026#39;password\u0026#39;); Note: you can also use charset=UTF-8, but sometimes it causes an error, so it\u0026rsquo;s better to use utf8.\nIf there is any connection error, it will throw a PDOException object that can be caught to handle Exception further.\nGood read: Connections and Connection management ¶\nYou can also pass in several driver options as an array to the fourth parameter. I recommend passing the parameter which puts PDO into exception mode. Because some PDO drivers don\u0026rsquo;t support native prepared statements, so PDO performs emulation of the prepare. It also lets you manually enable this emulation. To use the native server-side prepared statements, you should explicitly set it false.\nThe other is to turn off prepare emulation which is enabled in the MySQL driver by default, but prepare emulation should be turned off to use PDO safely.\nI will later explain why prepare emulation should be turned off. To find reason please check this post.\nIt is only usable if you are using an old version of MySQL which I do not recommended.\nBelow is an example of how you can do it:\n$db = new PDO(\u0026#39;mysql:host=localhost;dbname=testdb;charset=UTF-8\u0026#39;, \u0026#39;username\u0026#39;, \u0026#39;password\u0026#39;, array(PDO::ATTR_EMULATE_PREPARES =\u0026gt; false, PDO::ATTR_ERRMODE =\u0026gt; PDO::ERRMODE_EXCEPTION)); Can we set attributes after PDO construction?\nYes, we can also set some attributes after PDO construction with the setAttribute method:\n$db = new PDO(\u0026#39;mysql:host=localhost;dbname=testdb;charset=UTF-8\u0026#39;, \u0026#39;username\u0026#39;, \u0026#39;password\u0026#39;); $db-\u0026gt;setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION); $db-\u0026gt;setAttribute(PDO::ATTR_EMULATE_PREPARES, false); Error Handling Error handling is much easier in PDO than mysql_.\nA common practice when using mysql_ is:\n//Connected to MySQL $result = mysql_query(\u0026#34;SELECT * FROM table\u0026#34;, $link) or die(mysql_error($link)); OR die() is not a good way to handle the error since we can not handle the thing in die. It will just end the script abruptly and then echo the error to the screen which you usually do NOT want to show to your end users, and let bloody hackers discover your schema. Alternately, the return values of mysql_ functions can often be used in conjunction with mysql_error() to handle errors.\nPDO offers a better solution: exceptions. Anything we do with PDO should be wrapped in a try-catch block. We can force PDO into one of three error modes by setting the error mode attribute. Three error handling modes are below.\nPDO::ERRMODE_SILENT. It\u0026rsquo;s just setting error codes and acts pretty much the same as mysql_ where you must check each result and then look at $db-\u0026gt;errorInfo(); to get the error details. PDO::ERRMODE_WARNING Raise E_WARNING. (Run-time warnings (non-fatal errors). Execution of the script is not halted.) PDO::ERRMODE_EXCEPTION: Throw exceptions. It represents an error raised by PDO. You should not throw a PDOException from your own code. See Exceptions for more information about exceptions in PHP. It acts very much like or die(mysql_error());, when it isn\u0026rsquo;t caught. But unlike or die(), the PDOException can be caught and handled gracefully if you choose to do so. Good read:\nErrors and error handling ¶ The PDOException class ¶ Exceptions ¶ Like:\n$stmt-\u0026gt;setAttribute( PDO::ATTR_ERRMODE, PDO::ERRMODE_SILENT ); $stmt-\u0026gt;setAttribute( PDO::ATTR_ERRMODE, PDO::ERRMODE_WARNING ); $stmt-\u0026gt;setAttribute( PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION ); And you can wrap it in try-catch, like below:\ntry { //Connect as appropriate as above $db-\u0026gt;query(\u0026#39;hi\u0026#39;); //Invalid query! } catch (PDOException $ex) { echo \u0026#34;An Error occured!\u0026#34;; //User friendly message/message you want to show to user some_logging_function($ex-\u0026gt;getMessage()); } You do not have to handle with try-catch right now. You can catch it at any time appropriate, but I strongly recommend you to use try-catch. Also it may make more sense to catch it at outside the function that calls the PDO stuff:\nfunction data_fun($db) { $stmt = $db-\u0026gt;query(\u0026#34;SELECT * FROM table\u0026#34;); return $stmt-\u0026gt;fetchAll(PDO::FETCH_ASSOC); } //Then later try { data_fun($db); } catch(PDOException $ex) { //Here you can handle error and show message/perform action you want. } Also, you can handle by or die() or we can say like mysql_, but it will be really varied. You can hide the dangerous error messages in production by turning display_errors off and just reading your error log.\nNow, after reading all the things above, you are probably thinking: what the heck is that when I just want to start leaning simple SELECT, INSERT, UPDATE, or DELETE statements? Don\u0026rsquo;t worry, here we go:\nSelecting Data So what you are doing in mysql_ is:\n\u0026lt;?php $result = mysql_query(\u0026#39;SELECT * from table\u0026#39;) or die(mysql_error()); $num_rows = mysql_num_rows($result); while($row = mysql_fetch_assoc($result)) { echo $row[\u0026#39;field1\u0026#39;]; } Now in PDO, you can do this like:\n\u0026lt;?php $stmt = $db-\u0026gt;query(\u0026#39;SELECT * FROM table\u0026#39;); while($row = $stmt-\u0026gt;fetch(PDO::FETCH_ASSOC)) { echo $row[\u0026#39;field1\u0026#39;]; } Or\n\u0026lt;?php $stmt = $db-\u0026gt;query(\u0026#39;SELECT * FROM table\u0026#39;); $results = $stmt-\u0026gt;fetchAll(PDO::FETCH_ASSOC); //Use $results Note: If you are using the method like below (query()), this method returns a PDOStatement object. So if you want to fetch the result, use it like above.\n\u0026lt;?php foreach($db-\u0026gt;query(\u0026#39;SELECT * FROM table\u0026#39;) as $row) { echo $row[\u0026#39;field1\u0026#39;]; } In PDO Data, it is obtained via the -\u0026gt;fetch(), a method of your statement handle. Before calling fetch, the best approach would be telling PDO how you’d like the data to be fetched. In the below section I am explaining this.\nInsert and Update or Delete statements What we are doing in mysql_ function is:\n\u0026lt;?php $results = mysql_query(\u0026#34;UPDATE table SET field=\u0026#39;value\u0026#39;\u0026#34;) or die(mysql_error()); echo mysql_affected_rows($result); And in pdo, this same thing can be done by:\n\u0026lt;?php $affected_rows = $db-\u0026gt;exec(\u0026#34;UPDATE table SET field=\u0026#39;value\u0026#39;\u0026#34;); echo $affected_rows; In the above query PDO::exec execute an SQL statement and returns the number of affected rows.\nInsert and delete will be covered later.\nThe above method is only useful when you are not using variable in query. But when you need to use a variable in a query, do not ever ever try like the above and there for prepared statement or parameterized statement is.\nOthers in PDO Fetch Modes in PDO Note the use of PDO::FETCH_ASSOC in the fetch() and fetchAll() code above. This tells PDO to return the rows as an associative array with the field names as keys. There are many other fetch modes too which I will explain one by one.\nFirst of all, I explain how to select fetch mode:\n$stmt-\u0026gt;fetch(PDO::FETCH_ASSOC) In the above, I have been using fetch(). You can also use:\nPDOStatement::fetchAll() - Returns an array containing all of the result set rows PDOStatement::fetchColumn() - Returns a single column from the next row of a result set PDOStatement::fetchObject() - Fetches the next row and returns it as an object. PDOStatement::setFetchMode() - Set the default fetch mode for this statement Now I come to fetch mode:\nPDO::FETCH_ASSOC: returns an array indexed by column name as returned in your result set PDO::FETCH_BOTH (default): returns an array indexed by both column name and 0-indexed column number as returned in your result set There are even more choices! Read about them all in PDOStatement Fetch documentation..\nPrepared Statements Q. What is a prepared statement and why do I need them? A. A prepared statement is a pre-compiled SQL statement that can be executed multiple times by sending only the data to the server.\nThe typical workflow of using a prepared statement is as follows (quoted from Wikipedia three 3 point):\nPrepare: The statement template is created by the application and sent to the database management system (DBMS). Certain values are left unspecified, called parameters, placeholders or bind variables (labelled ? below):\nINSERT INTO PRODUCT (name, price) VALUES (?, ?)\nThe DBMS parses, compiles, and performs query optimization on the statement template, and stores the result without executing it.\nExecute: At a later time, the application supplies (or binds) values for the parameters, and the DBMS executes the statement (possibly returning a result). The application may execute the statement as many times as it wants with different values. In this example, it might supply \u0026lsquo;Bread\u0026rsquo; for the first parameter and 1.00 for the second parameter.\nYou can use a prepared statement by including placeholders in your SQL. There are basically three ones without placeholders (don\u0026rsquo;t try this with variable its above one), one with unnamed placeholders, and one with named placeholders.\nQ. So now, what are named placeholders and how do I use them? A. Named placeholders. Use descriptive names preceded by a colon, instead of question marks. We don\u0026rsquo;t care about position/order of value in name place holder:\n$stmt-\u0026gt;bindParam(\u0026#39;:bla\u0026#39;, $bla); bindParam(parameter,variable,data_type,length,driver_options)\nYou can also bind using an execute array as well:\n\u0026lt;?php $stmt = $db-\u0026gt;prepare(\u0026#34;SELECT * FROM table WHERE id=:id AND name=:name\u0026#34;); $stmt-\u0026gt;execute(array(\u0026#39;:name\u0026#39; =\u0026gt; $name, \u0026#39;:id\u0026#39; =\u0026gt; $id)); $rows = $stmt-\u0026gt;fetchAll(PDO::FETCH_ASSOC); Another nice feature for OOP friends is that named placeholders have the ability to insert objects directly into your database, assuming the properties match the named fields. For example:\nclass person { public $name; public $add; function __construct($a,$b) { $this-\u0026gt;name = $a; $this-\u0026gt;add = $b; } } $demo = new person(\u0026#39;john\u0026#39;,\u0026#39;29 bla district\u0026#39;); $stmt = $db-\u0026gt;prepare(\u0026#34;INSERT INTO table (name, add) value (:name, :add)\u0026#34;); $stmt-\u0026gt;execute((array)$demo); Q. So now, what are unnamed placeholders and how do I use them? A. Let\u0026rsquo;s have an example:\n\u0026lt;?php $stmt = $db-\u0026gt;prepare(\u0026#34;INSERT INTO folks (name, add) values (?, ?)\u0026#34;); $stmt-\u0026gt;bindValue(1, $name, PDO::PARAM_STR); $stmt-\u0026gt;bindValue(2, $add, PDO::PARAM_STR); $stmt-\u0026gt;execute(); and\n$stmt = $db-\u0026gt;prepare(\u0026#34;INSERT INTO folks (name, add) values (?, ?)\u0026#34;); $stmt-\u0026gt;execute(array(\u0026#39;john\u0026#39;, \u0026#39;29 bla district\u0026#39;)); In the above, you can see those ? instead of a name like in a name place holder. Now in the first example, we assign variables to the various placeholders ($stmt-\u0026gt;bindValue(1, $name, PDO::PARAM_STR);). Then, we assign values to those placeholders and execute the statement. In the second example, the first array element goes to the first ? and the second to the second ?.\nNOTE: In unnamed placeholders we must take care of the proper order of the elements in the array that we are passing to the PDOStatement::execute() method.\nSELECT, INSERT, UPDATE, DELETE prepared queries SELECT:\n$stmt = $db-\u0026gt;prepare(\u0026#34;SELECT * FROM table WHERE id=:id AND name=:name\u0026#34;); $stmt-\u0026gt;execute(array(\u0026#39;:name\u0026#39; =\u0026gt; $name, \u0026#39;:id\u0026#39; =\u0026gt; $id)); $rows = $stmt-\u0026gt;fetchAll(PDO::FETCH_ASSOC); INSERT:\n$stmt = $db-\u0026gt;prepare(\u0026#34;INSERT INTO table(field1,field2) VALUES(:field1,:field2)\u0026#34;); $stmt-\u0026gt;execute(array(\u0026#39;:field1\u0026#39; =\u0026gt; $field1, \u0026#39;:field2\u0026#39; =\u0026gt; $field2)); $affected_rows = $stmt-\u0026gt;rowCount(); DELETE:\n$stmt = $db-\u0026gt;prepare(\u0026#34;DELETE FROM table WHERE id=:id\u0026#34;); $stmt-\u0026gt;bindValue(\u0026#39;:id\u0026#39;, $id, PDO::PARAM_STR); $stmt-\u0026gt;execute(); $affected_rows = $stmt-\u0026gt;rowCount(); UPDATE:\n$stmt = $db-\u0026gt;prepare(\u0026#34;UPDATE table SET name=? WHERE id=?\u0026#34;); $stmt-\u0026gt;execute(array($name, $id)); $affected_rows = $stmt-\u0026gt;rowCount(); NOTE: If you use the like statement, please bind the % in execute statement.\nGetting the row count Instead of using mysql_num_rows to get the number of returned rows, you can get a PDOStatement and do rowCount(), like:\n\u0026lt;?php $stmt = $db-\u0026gt;query(\u0026#39;SELECT * FROM table\u0026#39;); $row_count = $stmt-\u0026gt;rowCount(); echo $row_count.\u0026#39; rows selected\u0026#39;; Getting the Last Inserted ID \u0026lt;?php $result = $db-\u0026gt;exec(\u0026#34;INSERT INTO table(firstname, lastname) VAULES(\u0026#39;John\u0026#39;, \u0026#39;Doe\u0026#39;)\u0026#34;); $insertId = $db-\u0026gt;lastInsertId(); NOTE However PDO and/or MySQLi are not completely safe. Check the answer Are PDO prepared statements sufficient to prevent SQL injection? by ircmaxell. Also, I am quoting some part from his answer:\n$pdo-\u0026gt;setAttribute(PDO::ATTR_EMULATE_PREPARES, false); $pdo-\u0026gt;query(\u0026#39;SET NAMES GBK\u0026#39;); $stmt = $pdo-\u0026gt;prepare(\u0026#34;SELECT * FROM test WHERE name = ? LIMIT 1\u0026#34;); $stmt-\u0026gt;execute(array(chr(0xbf) . chr(0x27) . \u0026#34; OR 1=1 /*\u0026#34;)); Addition This article is excerpted from why-shouldnt-i-use-mysql-functions-in-php and revised appropriately.\n","date":"2021-02-14T14:26:56Z","image":"https://static.aecra.cn/cover/why-should-not-i-use-mysql-functions-in-php.jpg","permalink":"https://www.aecra.cn/en/article/why-should-not-i-use-mysql-functions-in-php/","title":"Why shouldn't I use mysql_* functions in PHP?"}]